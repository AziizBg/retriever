{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f59893a-5abe-4ce7-b23e-6d9800f9b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd       \n",
    "import re\n",
    "import networkx as nx     \n",
    "import warnings\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Configure settings for better display and fewer warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34b1749a-4129-4626-90d8-0ada158ba83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Read the API key\n",
    "key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "# Define the llm\n",
    "model_name = \"nvidia/llama-3.1-nemotron-ultra-253b-v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=key,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796efbd-a878-4bc7-8b0e-8a5be2b525b9",
   "metadata": {},
   "source": [
    "## Define the LLM Prompt for Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab4d9eb2-982e-407b-8155-dd8bd222660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a medical information-extraction assistant focused on respiratory diseases.\n",
    "Given a passage of text about {disease}, extract Subject-Predicate-Object (SPO) triples.\n",
    "\n",
    "Extraction rules:\n",
    "────────────────\n",
    "• Extract any medically meaningful relation related to {disease} and its context.\n",
    "• Subjects and objects can be: symptoms, causes, treatments, diagnoses, risk factors, complications, or related medical concepts.\n",
    "• Use only these predicates (verbatim, lowercase):\n",
    "  has_symptom, is_diagnosed_by, is_treated_by, is_caused_by, \n",
    "  is_prevented_by, has_risk_factor, leads_to_complication, \n",
    "  treats, causes, occurs_with, associated_with\n",
    "• Subjects/objects should be full terms — no pronouns or abbreviations.\n",
    "\n",
    "Output format:\n",
    "─────────────\n",
    "• CSV only. First line must be:\n",
    "  subject,predicate,object\n",
    "• No explanation, no blank lines, no quotation marks, no punctuation at end of terms.\n",
    "\"\"\"\n",
    "\n",
    "# User prompt template (for formatting with each chunk)\n",
    "user_prompt_template = \"\"\"\n",
    "EXTRACT SPO TRIPLES\n",
    "\n",
    "Disease context: {disease}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{text_chunk}\\\"\\\"\\\"\n",
    "\n",
    "Your response must be a CSV table starting with the header:\n",
    "subject,predicate,object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c11cc3e-9072-4323-9e24-3f01d8c3aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textualize_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def format_dict(d):\n",
    "        lines = []\n",
    "        for key, value in d.items():\n",
    "            if key == 'url':\n",
    "                continue\n",
    "            if isinstance(value, dict):\n",
    "                lines.append(f\"{key}:\\n{format_dict(value)}\")\n",
    "            else:\n",
    "                lines.append(f\"{key}:\\n{value}\")\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        text = format_dict(data)\n",
    "    elif isinstance(data, list):\n",
    "        text = '\\n\\n'.join(format_dict(item) for item in data)\n",
    "    else:\n",
    "        text = str(data)\n",
    "\n",
    "    return text\n",
    "\n",
    "def format_llm_output(output_text, save_path):\n",
    "    lines = output_text.strip().splitlines()\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.lower().startswith(\"subject\") or line.count(',') != 2:\n",
    "            continue\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if all(parts):\n",
    "            cleaned_lines.append(parts)\n",
    "\n",
    "    if not cleaned_lines:\n",
    "        print(f\"Warning: No valid SPO triples found in output: {save_path}\")\n",
    "        return\n",
    "        \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"subject\", \"predicate\", \"object\"])\n",
    "        writer.writerows(cleaned_lines)\n",
    "\n",
    "# Function to save LLM output text into cleaned CSV\n",
    "def save_llm_output(file_name, output_text, output_path):\n",
    "    file_path = os.path.join(output_path, f\"{file_name}.csv\")\n",
    "    format_llm_output(output_text, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef56c32-8a52-4512-9be3-058508f23972",
   "metadata": {},
   "source": [
    "## LLM Interaction - Extracting Triples (Chunk by Chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f9895c1-2b9a-4e70-98d5-942cc00c671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spo(raw_data_path, output_path, client):\n",
    "    failed_files = 0\n",
    "\n",
    "    folders = [f for f in os.listdir(raw_data_path) if os.path.isdir(os.path.join(raw_data_path, f))]\n",
    "\n",
    "    for folder_name in tqdm(folders, desc=\"Processing folders\"):\n",
    "        folder_path = os.path.join(raw_data_path, folder_name)\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file_name in tqdm(files, desc=f\"→ {folder_name}\", leave=False):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                raw_text = textualize_json(file_path)\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt.format(disease=folder_name)},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt_template.format(text_chunk=raw_text, disease=folder_name)}\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    stream=False\n",
    "                )\n",
    "\n",
    "                output_text = completion.choices[0].message.content\n",
    "                new_file_name = f\"{folder_name}_{os.path.splitext(file_name)[0]}\"\n",
    "                save_llm_output(new_file_name, output_text, output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name} in folder {folder_name}: {e}\")\n",
    "                failed_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06336b17-4c14-4422-994a-31936202c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "→ asthma:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "→ asthma:   9%|▉         | 1/11 [00:08<01:28,  8.85s/it]\u001b[A\n",
      "→ asthma:  18%|█▊        | 2/11 [00:34<02:48, 18.77s/it]\u001b[A\n",
      "→ asthma:  27%|██▋       | 3/11 [00:41<01:48, 13.53s/it]\u001b[A\n",
      "→ asthma:  36%|███▋      | 4/11 [01:17<02:34, 22.13s/it]\u001b[A\n",
      "→ asthma:  45%|████▌     | 5/11 [01:24<01:39, 16.62s/it]\u001b[A\n",
      "→ asthma:  55%|█████▍    | 6/11 [01:35<01:14, 14.82s/it]\u001b[A\n",
      "→ asthma:  64%|██████▎   | 7/11 [01:43<00:50, 12.50s/it]\u001b[A\n",
      "→ asthma:  73%|███████▎  | 8/11 [01:49<00:31, 10.67s/it]\u001b[A\n",
      "→ asthma:  82%|████████▏ | 9/11 [01:59<00:20, 10.49s/it]\u001b[A\n",
      "→ asthma:  91%|█████████ | 10/11 [02:07<00:09,  9.66s/it]\u001b[A\n",
      "→ asthma: 100%|██████████| 11/11 [02:25<00:00, 12.09s/it]\u001b[A\n",
      "Processing folders:  20%|██        | 1/5 [02:25<09:41, 145.31s/it]\n",
      "→ chronic-obstructive-pulmonary-disease-copd:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  11%|█         | 1/9 [00:21<02:48, 21.10s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  22%|██▏       | 2/9 [00:31<01:43, 14.72s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  33%|███▎      | 3/9 [00:40<01:14, 12.35s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  44%|████▍     | 4/9 [00:55<01:06, 13.30s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  56%|█████▌    | 5/9 [01:08<00:51, 13.00s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  67%|██████▋   | 6/9 [01:13<00:31, 10.54s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  78%|███████▊  | 7/9 [01:30<00:25, 12.52s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd:  89%|████████▉ | 8/9 [02:06<00:20, 20.16s/it]\u001b[A\n",
      "→ chronic-obstructive-pulmonary-disease-copd: 100%|██████████| 9/9 [02:26<00:00, 19.99s/it]\u001b[A\n",
      "Processing folders:  40%|████      | 2/5 [04:51<07:18, 146.06s/it]                         \u001b[A\n",
      "→ coronavirus-disease-(covid-19):   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  14%|█▍        | 1/7 [00:11<01:06, 11.08s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  29%|██▊       | 2/7 [00:22<00:57, 11.50s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  43%|████▎     | 3/7 [00:43<01:02, 15.72s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  57%|█████▋    | 4/7 [00:49<00:35, 11.95s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  71%|███████▏  | 5/7 [00:57<00:20, 10.38s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19):  86%|████████▌ | 6/7 [01:03<00:09,  9.02s/it]\u001b[A\n",
      "→ coronavirus-disease-(covid-19): 100%|██████████| 7/7 [01:30<00:00, 14.74s/it]\u001b[A\n",
      "Processing folders:  60%|██████    | 3/5 [06:22<04:01, 120.60s/it]             \u001b[A\n",
      "→ pneumonia:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "→ pneumonia:   9%|▉         | 1/11 [00:08<01:29,  8.95s/it]\u001b[A\n",
      "→ pneumonia:  18%|█▊        | 2/11 [00:12<00:52,  5.80s/it]\u001b[A\n",
      "→ pneumonia:  27%|██▋       | 3/11 [00:19<00:51,  6.44s/it]\u001b[A\n",
      "→ pneumonia:  36%|███▋      | 4/11 [00:24<00:40,  5.83s/it]\u001b[A\n",
      "→ pneumonia:  45%|████▌     | 5/11 [00:32<00:38,  6.41s/it]\u001b[A\n",
      "→ pneumonia:  55%|█████▍    | 6/11 [00:44<00:41,  8.35s/it]\u001b[A\n",
      "→ pneumonia:  64%|██████▎   | 7/11 [00:52<00:33,  8.35s/it]\u001b[A\n",
      "→ pneumonia:  73%|███████▎  | 8/11 [00:59<00:23,  7.88s/it]\u001b[A\n",
      "→ pneumonia:  82%|████████▏ | 9/11 [01:04<00:14,  7.05s/it]\u001b[A\n",
      "→ pneumonia:  91%|█████████ | 10/11 [01:14<00:07,  7.80s/it]\u001b[A\n",
      "→ pneumonia: 100%|██████████| 11/11 [01:26<00:00,  9.29s/it]\u001b[A\n",
      "Processing folders:  80%|████████  | 4/5 [07:48<01:47, 107.25s/it]\n",
      "→ tuberculosis:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "→ tuberculosis:  11%|█         | 1/9 [00:12<01:36, 12.11s/it]\u001b[A\n",
      "→ tuberculosis:  22%|██▏       | 2/9 [00:16<00:53,  7.67s/it]\u001b[A\n",
      "→ tuberculosis:  33%|███▎      | 3/9 [00:23<00:43,  7.23s/it]\u001b[A\n",
      "→ tuberculosis:  44%|████▍     | 4/9 [00:27<00:29,  5.91s/it]\u001b[A\n",
      "→ tuberculosis:  56%|█████▌    | 5/9 [00:35<00:26,  6.59s/it]\u001b[A\n",
      "→ tuberculosis:  67%|██████▋   | 6/9 [00:46<00:24,  8.14s/it]\u001b[A\n",
      "→ tuberculosis:  78%|███████▊  | 7/9 [00:54<00:16,  8.10s/it]\u001b[A\n",
      "→ tuberculosis:  89%|████████▉ | 8/9 [01:01<00:07,  7.75s/it]\u001b[A\n",
      "→ tuberculosis: 100%|██████████| 9/9 [01:12<00:00,  8.85s/it]\u001b[A\n",
      "Processing folders: 100%|██████████| 5/5 [09:01<00:00, 108.30s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_spo('./raw_files', './spo/raw', client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689f813-507d-4e4a-bfba-86b59bcef508",
   "metadata": {},
   "source": [
    "### Extraction Summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06c18ac4-6add-4eaf-b5b2-8a7c679a8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asthma: 677 SPO triples\n",
      "chronic-obstructive-pulmonary-disease-copd: 459 SPO triples\n",
      "coronavirus-disease-(covid-19): 283 SPO triples\n",
      "pneumonia: 373 SPO triples\n",
      "tuberculosis: 298 SPO triples\n",
      "\n",
      "Total SPO triples: 2090\n",
      "Total diseases: 5\n"
     ]
    }
   ],
   "source": [
    "def count_spos_per_disease(spo_folder_path):\n",
    "    spo_dict = defaultdict(list)\n",
    "\n",
    "    for file_name in os.listdir(spo_folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            disease = file_name.split('_')[0]\n",
    "            file_path = os.path.join(spo_folder_path, file_name)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if {'subject', 'predicate', 'object'}.issubset(df.columns):\n",
    "                    for _, row in df.iterrows():\n",
    "                        spo_dict[disease].append((row[\"subject\"], row[\"predicate\"], row[\"object\"]))\n",
    "                else:\n",
    "                    print(f\"Skipped file (missing columns): {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_name}: {e}\")\n",
    "\n",
    "    diseases = []\n",
    "    counts = []\n",
    "    # Optional: Print summary\n",
    "    for disease, spo_list in sorted(spo_dict.items()):\n",
    "        diseases.append(disease)\n",
    "        counts.append(len(spo_list))\n",
    "        print(f\"{disease}: {len(spo_list)} SPO triples\")\n",
    "\n",
    "    total_spo = sum(len(v) for v in spo_dict.values())\n",
    "    print(f\"\\nTotal SPO triples: {total_spo}\")\n",
    "    print(f\"Total diseases: {len(spo_dict)}\")\n",
    "\n",
    "    return dict(spo_dict), diseases, counts\n",
    "\n",
    "all_extracted_triples, diseases, original_counts = count_spos_per_disease('./spo/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c80159-56cd-45db-b3e6-18724d636d84",
   "metadata": {},
   "source": [
    "## Post-processing & canonicalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9eecdd-4c06-4d9b-8b54-d176642fa1f1",
   "metadata": {},
   "source": [
    "### Normalize and De-duplicate Triples\n",
    "\n",
    "Processes each disease separately.  \n",
    "Normalizes all triples (trimming and lowercasing).  \n",
    "Deduplicates based on the cleaned (subject, predicate, object) tuples.  \n",
    "Tracks and prints detailed stats per disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82b709d7-6698-43dc-9f7f-4dc79de61393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication Summary Per Disease:\n",
      "asthma: Kept 533 / 677 | Duplicates: 144, Empty: 0\n",
      "chronic-obstructive-pulmonary-disease-copd: Kept 418 / 459 | Duplicates: 41, Empty: 0\n",
      "coronavirus-disease-(covid-19): Kept 271 / 283 | Duplicates: 12, Empty: 0\n",
      "pneumonia: Kept 328 / 373 | Duplicates: 45, Empty: 0\n",
      "tuberculosis: Kept 279 / 298 | Duplicates: 19, Empty: 0\n",
      "Total remaining SPO: 1829\n"
     ]
    }
   ],
   "source": [
    "normalized_spo_by_disease = {}\n",
    "dedup_stats = {}\n",
    "\n",
    "for disease in diseases:\n",
    "    triples = all_extracted_triples[disease]\n",
    "\n",
    "    normalized_triples = []\n",
    "    seen_triples = set()\n",
    "    empty_removed_count = 0\n",
    "    duplicates_removed_count = 0\n",
    "\n",
    "    for i, (s, p, o) in enumerate(triples):\n",
    "        s = s.strip().lower() if isinstance(s, str) else ''\n",
    "        p = re.sub(r'\\s+', ' ', p.strip().lower()) if isinstance(p, str) else ''\n",
    "        o = o.strip().lower() if isinstance(o, str) else ''\n",
    "\n",
    "        if all([s, p, o]):\n",
    "            key = (s, p, o)\n",
    "            if key not in seen_triples:\n",
    "                normalized_triples.append({'subject': s, 'predicate': p, 'object': o})\n",
    "                seen_triples.add(key)\n",
    "            else:\n",
    "                duplicates_removed_count += 1\n",
    "        else:\n",
    "            empty_removed_count += 1\n",
    "\n",
    "    normalized_spo_by_disease[disease] = normalized_triples\n",
    "    dedup_stats[disease] = {\n",
    "        \"original\": len(triples),\n",
    "        \"kept\": len(normalized_triples),\n",
    "        \"duplicates_removed\": duplicates_removed_count,\n",
    "        \"empty_removed\": empty_removed_count\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "total = 0\n",
    "print(\"Deduplication Summary Per Disease:\")\n",
    "for disease in sorted(dedup_stats):\n",
    "    total += stats['kept']\n",
    "    stats = dedup_stats[disease]\n",
    "    print(f\"{disease}: Kept {stats['kept']} / {stats['original']} | Duplicates: {stats['duplicates_removed']}, Empty: {stats['empty_removed']}\")\n",
    "print(f\"Total remaining SPO: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f2325bd1-36a0-4150-8556-9a25d09810f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported SPO CSV files to: ./spo_normalized&deduplicated\n"
     ]
    }
   ],
   "source": [
    "def export_spo_per_disease(normalized_spo_by_disease, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for disease, triples in normalized_spo_by_disease.items():\n",
    "        filename = f\"{disease.lower().replace(' ', '_')}.csv\"\n",
    "        file_path = os.path.join(path, filename)\n",
    "\n",
    "        with open(file_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"subject\", \"predicate\", \"object\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(triples)\n",
    "    print(f\"Exported SPO CSV files to: {path}\")\n",
    "\n",
    "export_spo_per_disease(normalized_spo_by_disease, './spo/normalized&deduplicated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c317a-5ae4-430d-827f-420f31862288",
   "metadata": {},
   "source": [
    "### Refine the spo with another llm call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bcee506-f354-431f-9c7c-2008412ff62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refining_system_prompt = \"\"\"\n",
    "You are a medical SPO-triple refinement assistant for respiratory-disease knowledge graphs.\n",
    "\n",
    "Task\n",
    "────\n",
    "Given SPO triples about the disease {disease}, return a single, clean, fully-connected graph:\n",
    "\n",
    "• Eliminate semantic duplicates.  \n",
    "• Clarify vague terms (use formal medical wording, no abbreviations or pronouns).  \n",
    "• Ensure every node belongs to ONE connected component anchored on {disease}.  \n",
    "  – If a node is isolated but clearly relates to {disease}, ADD one factual triple to link it.  \n",
    "  – If connection is uncertain, leave the triple unchanged (do NOT fabricate facts).  \n",
    "• All text must be lowercase.\n",
    "\n",
    "Output format\n",
    "─────────────\n",
    "csv only — first line literally:\n",
    "subject,predicate,object\n",
    "No explanations, blank lines, quotes, or trailing punctuation.\n",
    "\"\"\"\n",
    "\n",
    "refining_user_prompt_template = \"\"\"\n",
    "REFINE SPO TRIPLES  –  DISEASE: {disease}\n",
    "\n",
    "Below is a csv table of extracted triples.\n",
    "\n",
    "Your tasks\n",
    "──────────\n",
    "1. Deduplicate rows that convey the same fact.  \n",
    "2. Rephrase unclear wording for precision.  \n",
    "3. Ensure every node appears in at least one edge connected (directly or indirectly) to \"{disease}\".  \n",
    "4. If a node is isolated yet obviously related, add ONE triple using an allowed predicate to connect it.  \n",
    "5. Do NOT invent new medical facts.  \n",
    "6. Return lowercase csv, starting with the header.\n",
    "\n",
    "Input triples:\n",
    "{csv_triples}\n",
    "\n",
    "Your output must start with:\n",
    "subject,predicate,object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "51d93db6-3d9e-4c8b-a675-3b8f7e4e7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_rows(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return list(reader)\n",
    "\n",
    "def rows_to_csv_string(rows):\n",
    "    output = [\"subject,predicate,object\"]\n",
    "    for row in rows:\n",
    "        output.append(f\"{row['subject']},{row['predicate']},{row['object']}\")\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def adaptive_chunks(rows, max_chars):\n",
    "    \"\"\"Yield batches of rows such that the total character length of the CSV string stays below max_chars.\"\"\"\n",
    "    batch = []\n",
    "    current_length = len(\"subject,predicate,object\\n\")  # header\n",
    "\n",
    "    for row in rows:\n",
    "        row_str = f\"{row['subject']},{row['predicate']},{row['object']}\\n\"\n",
    "        if current_length + len(row_str) > max_chars:\n",
    "            if batch:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                current_length = len(\"subject,predicate,object\\n\")\n",
    "        batch.append(row)\n",
    "        current_length += len(row_str)\n",
    "\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def refine_spo_from_csv(input_path, output_path, client, model_name, system_prompt, user_prompt_template, max_chars_per_batch=5000):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    failed_files = 0\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_path), desc=\"Refining SPO per disease\"):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        disease = os.path.splitext(file_name)[0].replace('_', ' ')\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "        try:\n",
    "            rows = read_csv_rows(file_path)\n",
    "            all_refined_rows = []\n",
    "\n",
    "            for i, batch in enumerate(adaptive_chunks(rows, max_chars=max_chars_per_batch)):\n",
    "                csv_chunk = rows_to_csv_string(batch)\n",
    "\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt.format(disease=disease)},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt_template.format(disease=disease, csv_triples=csv_chunk)}\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    stream=False\n",
    "                )\n",
    "\n",
    "                output_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "                # Parse the refined triples\n",
    "                lines = output_text.splitlines()\n",
    "                for line in lines[1:]:  # Skip header\n",
    "                    parts = line.split(\",\")\n",
    "                    if len(parts) == 3:\n",
    "                        subject, predicate, object_ = [p.strip() for p in parts]\n",
    "                        all_refined_rows.append({\n",
    "                            \"subject\": subject,\n",
    "                            \"predicate\": predicate,\n",
    "                            \"object\": object_\n",
    "                        })\n",
    "\n",
    "            # Deduplicate after merging batches\n",
    "            unique_rows = [dict(t) for t in {tuple(d.items()) for d in all_refined_rows}]\n",
    "\n",
    "            # Save final merged and deduplicated file\n",
    "            out_filename = f\"{file_name.replace('.csv', '')}_refined.csv\"\n",
    "            out_path = os.path.join(output_path, out_filename)\n",
    "\n",
    "            with open(out_path, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=[\"subject\", \"predicate\", \"object\"])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(unique_rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_name}: {e}\")\n",
    "            failed_files += 1\n",
    "\n",
    "    print(f\"\\n✅ Completed refinement with {failed_files} failure(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "16a6f4b0-bcdb-4f59-9d13-509871c3e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refining SPO per disease: 100%|██████████| 5/5 [10:06<00:00, 121.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Completed refinement with 0 failure(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "refine_spo_from_csv(\n",
    "    input_path='./spo/normalized&deduplicated',\n",
    "    output_path='./spo/refined',\n",
    "    client=client,  \n",
    "    model_name=model_name,\n",
    "    system_prompt=refining_system_prompt,\n",
    "    user_prompt_template=refining_user_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "78846483-38d1-4d56-986e-fd7707e65dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease                                               Raw  Refined      Δ\n",
      "----------------------------------------------------\n",
      "asthma                                                533      570     37\n",
      "chronic-obstructive-pulmonary-disease-copd            418      403    -15\n",
      "coronavirus-disease-(covid-19)                        271      228    -43\n",
      "pneumonia                                             328      562    234\n",
      "tuberculosis                                          279      367     88\n",
      "----------------------------------------------------\n",
      "Total                                                1829     2130    301\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_refinement_process(raw_path, refined_path):\n",
    "    summary = []\n",
    "\n",
    "    # Helper function to count SPO triples from a folder\n",
    "    def load_spo_counts(folder_path):\n",
    "        spo_counts = {}\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.csv'):\n",
    "                disease = file_name.replace('_refined', '').replace('.csv', '').replace('_', ' ')\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if {'subject', 'predicate', 'object'}.issubset(df.columns):\n",
    "                        spo_counts[disease] = len(df)\n",
    "                    else:\n",
    "                        print(f\"Skipped file (missing columns): {file_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_name}: {e}\")\n",
    "        return spo_counts\n",
    "\n",
    "    raw_counts = load_spo_counts(raw_path)\n",
    "    refined_counts = load_spo_counts(refined_path)\n",
    "\n",
    "    diseases = sorted(set(raw_counts.keys()).union(refined_counts.keys()))\n",
    "\n",
    "    print(f\"{'Disease':<50} {'Raw':>6} {'Refined':>8} {'Δ':>6}\")\n",
    "    print(\"-\" * 52)\n",
    "\n",
    "    total_raw = total_refined = 0\n",
    "\n",
    "    for disease in diseases:\n",
    "        raw = raw_counts.get(disease, 0)\n",
    "        refined = refined_counts.get(disease, 0)\n",
    "        delta = refined - raw\n",
    "        total_raw += raw\n",
    "        total_refined += refined\n",
    "\n",
    "        print(f\"{disease:<50} {raw:>6} {refined:>8} {delta:>6}\")\n",
    "        summary.append({\n",
    "            \"disease\": disease,\n",
    "            \"raw\": raw,\n",
    "            \"refined\": refined,\n",
    "            \"delta\": delta\n",
    "        })\n",
    "\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'Total':<50} {total_raw:>6} {total_refined:>8} {total_refined - total_raw:>6}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "summary = summarize_refinement_process(\n",
    "    raw_path='./spo/normalized&deduplicated',\n",
    "    refined_path='./spo/refined'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170123ff-b4b7-49a9-a2f0-c41d1bfc9c82",
   "metadata": {},
   "source": [
    "### graph count and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e828b1b3-6145-4e1f-890e-491c701c61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graphs: 100%|██████████| 5/5 [00:00<00:00, 96.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asthma                                        | graph  1 | 435 nodes | 570 edges\n",
      "chronic-obstructive-pulmonary-disease-copd    | graph  1 | 316 nodes | 403 edges\n",
      "coronavirus-disease-(covid-19)                | graph  1 | 197 nodes | 228 edges\n",
      "pneumonia                                     | graph  1 | 420 nodes | 562 edges\n",
      "tuberculosis                                  | graph  1 | 285 nodes | 367 edges\n",
      "\n",
      "Summary by disease\n",
      "disease                                        #graphs      nodes      edges\n",
      "-------------------------------------------------------\n",
      "asthma                                               1        435        570\n",
      "chronic-obstructive-pulmonary-disease-copd           1        316        403\n",
      "coronavirus-disease-(covid-19)                       1        197        228\n",
      "pneumonia                                            1        420        562\n",
      "tuberculosis                                         1        285        367\n",
      "-------------------------------------------------------\n",
      "TOTAL                                                5       1653       2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv_rows(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def csv_file_to_disease(file_name: str) -> str:\n",
    "    \"\"\"Assumes pattern '<disease>_refined.csv'. Adjust if needed.\"\"\"\n",
    "    return file_name.replace(\"_refined.csv\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def extract_graphs_from_directory(\n",
    "    input_folder: str,\n",
    "    output_json_path: str,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    all_graphs = []                   # list of all sub-graphs (for JSON)\n",
    "    disease_stats = defaultdict(lambda: {\"graphs\": 0, \"nodes\": 0, \"edges\": 0})\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_folder), desc=\"Building graphs\"):\n",
    "        if not file_name.endswith(\"_refined.csv\"):\n",
    "            continue\n",
    "\n",
    "        disease = csv_file_to_disease(file_name)\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        rows = read_csv_rows(file_path)\n",
    "\n",
    "        # Build MultiDiGraph\n",
    "        G = nx.MultiDiGraph()\n",
    "        for row in rows:\n",
    "            G.add_edge(row[\"subject\"], row[\"object\"], predicate=row[\"predicate\"])\n",
    "\n",
    "        # Weakly connected components = individual sub-graphs\n",
    "        for comp_id, comp_nodes in enumerate(nx.weakly_connected_components(G), 1):\n",
    "            sub = G.subgraph(comp_nodes)\n",
    "            triples = [\n",
    "                [u, d[\"predicate\"], v] for u, v, d in sub.edges(data=True)\n",
    "            ]\n",
    "\n",
    "            # Stats\n",
    "            n_nodes = sub.number_of_nodes()\n",
    "            n_edges = sub.number_of_edges()\n",
    "\n",
    "            disease_stats[disease][\"graphs\"] += 1\n",
    "            disease_stats[disease][\"nodes\"] += n_nodes\n",
    "            disease_stats[disease][\"edges\"] += n_edges\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{disease:<45} | graph {comp_id:>2} | \"\n",
    "                    f\"{n_nodes:>3} nodes | {n_edges:>3} edges\"\n",
    "                )\n",
    "\n",
    "            all_graphs.append(triples)\n",
    "\n",
    "    # Write the list-of-lists JSON\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(all_graphs, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nSummary by disease\")\n",
    "    print(f\"{'disease':<45} {'#graphs':>8} {'nodes':>10} {'edges':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    total_g = total_n = total_e = 0\n",
    "    for dis, s in sorted(disease_stats.items()):\n",
    "        print(\n",
    "            f\"{dis:<45} {s['graphs']:>8} {s['nodes']:>10} {s['edges']:>10}\"\n",
    "        )\n",
    "        total_g += s[\"graphs\"]\n",
    "        total_n += s[\"nodes\"]\n",
    "        total_e += s[\"edges\"]\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'TOTAL':<45} {total_g:>8} {total_n:>10} {total_e:>10}\")\n",
    "\n",
    "    return all_graphs, disease_stats\n",
    "\n",
    "\n",
    "graphs = extract_graphs_from_directory(\n",
    "    input_folder='./spo/refined',\n",
    "    output_json_path='./spo/graphs.json',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acafe6-7b96-42c1-9905-785ee2c8b688",
   "metadata": {},
   "source": [
    "## Store refined Triplets in neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "00ade06e-81c5-4454-9b0d-13f3aafce144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "# \n",
    "# uri = \"bolt://localhost:7687\"  # Neo4j Desktop default\n",
    "# user = \"neo4j\"\n",
    "# password = \"password\"  # Set this when you created your local DB\n",
    "# \n",
    "# driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "# \n",
    "# def add_triple(tx, subj, pred, obj):\n",
    "#     tx.run(\"\"\"\n",
    "#         MERGE (s:Entity {name: $subj})\n",
    "#         MERGE (o:Entity {name: $obj})\n",
    "#         MERGE (s)-[:RELATION {type: $pred}]->(o)\n",
    "#     \"\"\", subj=subj, obj=obj, pred=pred)\n",
    "# \n",
    "# with driver.session() as session:\n",
    "#     for subj, pred, obj in refined_triples:\n",
    "#         session.write_transaction(add_triple, subj, pred, obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG)",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

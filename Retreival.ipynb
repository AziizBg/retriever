{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AziizBg/retriever/blob/synthetic_data/Retreival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AziizBg/retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7GkLsWfmA_T",
        "outputId": "bbdb852b-fc33-452b-e784-03bb6eab4824"
      },
      "id": "q7GkLsWfmA_T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'retriever'...\n",
            "remote: Enumerating objects: 257, done.\u001b[K\n",
            "remote: Counting objects: 100% (257/257), done.\u001b[K\n",
            "remote: Compressing objects: 100% (209/209), done.\u001b[K\n",
            "remote: Total 257 (delta 75), reused 213 (delta 47), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (257/257), 8.24 MiB | 12.50 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBt9lw5jmLm6",
        "outputId": "015ca69f-4c9a-4584-f05b-ce9985a7afb6"
      },
      "id": "zBt9lw5jmLm6",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/retriever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "da89211a-d258-440e-b6a3-0c16234e99b2",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da89211a-d258-440e-b6a3-0c16234e99b2",
        "outputId": "94d76d05-78b2-44ef-e577-567e1127fc8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
            "Collecting torch==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.16.0 (from -r requirements.txt (line 2))\n",
            "  Using cached torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.1.0 (from -r requirements.txt (line 3))\n",
            "  Using cached torchaudio-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting torch-scatter (from -r requirements.txt (line 6))\n",
            "  Using cached https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_scatter-2.1.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (500 kB)\n",
            "Collecting torch-sparse (from -r requirements.txt (line 7))\n",
            "  Using cached https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_sparse-0.6.18%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "Collecting torch-cluster (from -r requirements.txt (line 8))\n",
            "  Using cached https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_cluster-1.6.3%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (753 kB)\n",
            "Collecting torch-spline-conv (from -r requirements.txt (line 9))\n",
            "  Using cached https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt21cpu-cp311-cp311-linux_x86_64.whl (210 kB)\n",
            "Collecting torch-geometric (from -r requirements.txt (line 11))\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting transformers==4.39.3 (from -r requirements.txt (line 13))\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "Collecting peft==0.10.0 (from -r requirements.txt (line 14))\n",
            "  Using cached peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.2.2)\n",
            "Collecting ogb (from -r requirements.txt (line 16))\n",
            "  Using cached ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.19.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (2.14.4)\n",
            "Collecting pcst_fast (from -r requirements.txt (line 20))\n",
            "  Using cached pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting gensim (from -r requirements.txt (line 21))\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy==1.12 (from -r requirements.txt (line 22))\n",
            "  Using cached scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (5.29.4)\n",
            "Collecting pymilvus (from -r requirements.txt (line 24))\n",
            "  Using cached pymilvus-2.5.10-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (2.32.3)\n",
            "Collecting python-dotenv>=1.0.0 (from -r requirements.txt (line 27))\n",
            "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0->-r requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (0.31.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (2024.11.6)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->-r requirements.txt (line 13))\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3->-r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0->-r requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0->-r requirements.txt (line 14)) (1.7.0)\n",
            "Collecting numpy (from torchvision==0.16.0->-r requirements.txt (line 2))\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->-r requirements.txt (line 1)) (12.5.82)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric->-r requirements.txt (line 11)) (3.11.15)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric->-r requirements.txt (line 11)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 15)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 15)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 15)) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb->-r requirements.txt (line 16)) (1.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb->-r requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb->-r requirements.txt (line 16)) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb->-r requirements.txt (line 16))\n",
            "  Using cached outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (4.3.8)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (2.11.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 17)) (75.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 19)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 19)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 19)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 19)) (0.70.15)\n",
            "Collecting pybind11>=2.1.0 (from pcst_fast->-r requirements.txt (line 20))\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->-r requirements.txt (line 21)) (7.1.0)\n",
            "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus->-r requirements.txt (line 24))\n",
            "  Using cached grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting ujson>=2.0.0 (from pymilvus->-r requirements.txt (line 24))\n",
            "  Using cached ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting milvus-lite>=2.4.0 (from pymilvus->-r requirements.txt (line 24))\n",
            "  Using cached milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 26)) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric->-r requirements.txt (line 11)) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17)) (4.0.12)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb->-r requirements.txt (line 16))\n",
            "  Using cached littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb->-r requirements.txt (line 16)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb->-r requirements.txt (line 16)) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 21)) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17)) (5.0.2)\n",
            "Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.1.0-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymilvus-2.5.10-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl (45.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: ujson, triton, torch-spline-conv, torch-scatter, python-dotenv, pybind11, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, milvus-lite, littleutils, grpcio, scipy, pcst_fast, outdated, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch-sparse, torch-geometric, torch-cluster, torch, tokenizers, pymilvus, gensim, transformers, torchvision, torchaudio, ogb, peft\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.67.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 grpcio-1.67.1 littleutils-0.2.4 milvus-lite-2.4.12 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 ogb-1.3.6 outdated-0.2.2 pcst_fast-1.0.10 peft-0.10.0 pybind11-2.13.6 pymilvus-2.5.10 python-dotenv-1.1.0 scipy-1.12.0 tokenizers-0.15.2 torch-2.1.0 torch-cluster-1.6.3+pt21cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt21cpu torch-sparse-0.6.18+pt21cpu torch-spline-conv-1.2.2+pt21cpu torchaudio-2.1.0 torchvision-0.16.0 transformers-4.39.3 triton-2.1.0 ujson-5.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout synthetic_data"
      ],
      "metadata": {
        "id": "wT5VlY9q4Uq2",
        "outputId": "7b0343aa-f968-4ca6-9e22-8943e5f0b2ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wT5VlY9q4Uq2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'synthetic_data'\n",
            "Your branch is up to date with 'origin/synthetic_data'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2dc4b816-97ef-4910-8f6b-83c67f19dcf7",
      "metadata": {
        "id": "2dc4b816-97ef-4910-8f6b-83c67f19dcf7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import json\n",
        "import gensim\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from pcst_fast import pcst_fast\n",
        "import re\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfcddaf-1b0b-4e77-ae31-61d190f01748",
      "metadata": {
        "id": "5dfcddaf-1b0b-4e77-ae31-61d190f01748"
      },
      "source": [
        "# Load embedding modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "893d4130-c2ff-4c0b-b48c-768307a4bfa4",
      "metadata": {
        "id": "893d4130-c2ff-4c0b-b48c-768307a4bfa4"
      },
      "outputs": [],
      "source": [
        "pretrained_repo = 'sentence-transformers/all-roberta-large-v1'\n",
        "batch_size = 1024  # Adjust the batch size as needed\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids=None, attention_mask=None):\n",
        "        super().__init__()\n",
        "        self.data = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"att_mask\": attention_mask,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data[\"input_ids\"].size(0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if isinstance(index, torch.Tensor):\n",
        "            index = index.item()\n",
        "        batch_data = dict()\n",
        "        for key in self.data.keys():\n",
        "            if self.data[key] is not None:\n",
        "                batch_data[key] = self.data[key][index]\n",
        "        return batch_data\n",
        "\n",
        "class Sentence_Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_repo):\n",
        "        super(Sentence_Transformer, self).__init__()\n",
        "        print(f\"inherit model weights from {pretrained_repo}\")\n",
        "        self.bert_model = AutoModel.from_pretrained(pretrained_repo)\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
        "        data_type = token_embeddings.dtype\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(data_type)\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def forward(self, input_ids, att_mask):\n",
        "        bert_out = self.bert_model(input_ids=input_ids, attention_mask=att_mask)\n",
        "        sentence_embeddings = self.mean_pooling(bert_out, att_mask)\n",
        "\n",
        "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "        return sentence_embeddings\n",
        "\n",
        "def load_sbert():\n",
        "\n",
        "    model = Sentence_Transformer(pretrained_repo)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_repo)\n",
        "\n",
        "    # data parallel\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f'Using {torch.cuda.device_count()} GPUs')\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, tokenizer, device\n",
        "\n",
        "\n",
        "def sbert_text2embedding(model, tokenizer, device, text):\n",
        "    if len(text) == 0:\n",
        "        return torch.zeros((0, 1024))\n",
        "\n",
        "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    dataset = Dataset(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask)\n",
        "\n",
        "    # DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Placeholder for storing the embeddings\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Iterate through batches\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in dataloader:\n",
        "            # Move batch to the appropriate device\n",
        "            batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            embeddings = model(input_ids=batch[\"input_ids\"], att_mask=batch[\"att_mask\"])\n",
        "\n",
        "            # Append the embeddings to the list\n",
        "            all_embeddings.append(embeddings)\n",
        "\n",
        "    # Concatenate the embeddings from all batches\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0).cpu()\n",
        "\n",
        "    return all_embeddings\n",
        "\n",
        "load_model = {\n",
        "    'sbert': load_sbert,\n",
        "}\n",
        "\n",
        "load_text2embedding = {\n",
        "    'sbert': sbert_text2embedding,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a187b1d0-b39d-4639-bfca-fbfc503a1245",
      "metadata": {
        "id": "a187b1d0-b39d-4639-bfca-fbfc503a1245"
      },
      "source": [
        "# Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ODrNyQC0wVr"
      },
      "id": "0ODrNyQC0wVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2d7415b6-98ed-4e43-adf8-d912f36abf45",
      "metadata": {
        "id": "2d7415b6-98ed-4e43-adf8-d912f36abf45"
      },
      "outputs": [],
      "source": [
        "model_name = 'sbert'\n",
        "path = 'dataset'\n",
        "path_nodes = f'{path}/nodes'\n",
        "path_edges = f'{path}/edges'\n",
        "path_graphs = f'{path}/graphs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e89fb80-6df3-4c6d-8db4-5964d11915f8",
      "metadata": {
        "id": "6e89fb80-6df3-4c6d-8db4-5964d11915f8"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient, DataType, FieldSchema, CollectionSchema\n",
        "\n",
        "CLUSTER_ENDPOINT = (\n",
        "    \"https://in03-7a5f9d2a1aa84ef.serverless.gcp-us-west1.cloud.zilliz.com\"\n",
        ")\n",
        "API_KEY = (\n",
        "    \"a73c79fb1924d05aeb410abc0d5669293cc33be37a123953be640725aa42198ef5c1e499cc07f231977c742ad6e6977c6eddec05\"\n",
        ")\n",
        "\n",
        "milvus_client = MilvusClient(uri=CLUSTER_ENDPOINT, token=API_KEY)\n",
        "\n",
        "COLLECTION_NAME = \"graph_embeddings\"\n",
        "VECTOR_DIM      = 1024          # SBERT default; change if yours differs\n",
        "METRIC_TYPE     = \"COSINE\"     # or \"IP\" for inner-product\n",
        "\n",
        "# Drop the collection if it already exists (for a clean slate)\n",
        "if COLLECTION_NAME in milvus_client.list_collections():\n",
        "    milvus_client.drop_collection(COLLECTION_NAME)\n",
        "\n",
        "# Define the schema\n",
        "schema = CollectionSchema(\n",
        "    fields=[\n",
        "        FieldSchema(\n",
        "            name=\"graph_id\",\n",
        "            dtype=DataType.INT64,\n",
        "            is_primary=True,\n",
        "            auto_id=False\n",
        "        ),\n",
        "        FieldSchema(\n",
        "            name=\"embedding\",\n",
        "            dtype=DataType.FLOAT_VECTOR,\n",
        "            dim=VECTOR_DIM\n",
        "        ),\n",
        "        FieldSchema(                 # optional metadata field\n",
        "            name=\"graph_idx\",\n",
        "            dtype=DataType.INT64\n",
        "        )\n",
        "    ],\n",
        "    description=\"Mean-pooled SBERT embeddings of knowledge graphs\"\n",
        ")\n",
        "\n",
        "index_params = milvus_client.prepare_index_params()\n",
        "index_params.add_index(\"embedding\", index_type=\"IVF_FLAT\", metric_type=\"COSINE\", index_params={\"nlist\": 64})\n",
        "milvus_client.create_collection(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    schema=schema,\n",
        "    consistency_level=\"Strong\",\n",
        "    index_params=index_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c868ece-0fe5-4197-88f8-c5cdfad18fb4",
      "metadata": {
        "id": "9c868ece-0fe5-4197-88f8-c5cdfad18fb4"
      },
      "outputs": [],
      "source": [
        "def preprocessing_step_one():\n",
        "    # Load the graphs from the JSON file\n",
        "    with open('graphs.json', 'r') as f:\n",
        "        graphs = json.load(f)\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(path_nodes, exist_ok=True)\n",
        "    os.makedirs(path_edges, exist_ok=True)\n",
        "\n",
        "    # Process each graph\n",
        "    for i, triples in enumerate(tqdm(graphs)):\n",
        "        node_map = {}   # Maps node label → node ID\n",
        "        edges = []\n",
        "\n",
        "        for h, r, t in triples:\n",
        "            h = h.lower()\n",
        "            t = t.lower()\n",
        "            if h not in node_map:\n",
        "                node_map[h] = len(node_map)\n",
        "            if t not in node_map:\n",
        "                node_map[t] = len(node_map)\n",
        "            edges.append({'src': node_map[h], 'edge_attr': r, 'dst': node_map[t]})\n",
        "\n",
        "        # Convert node map to DataFrame\n",
        "        nodes_df = pd.DataFrame(\n",
        "            [{'node_id': v, 'node_attr': k} for k, v in node_map.items()],\n",
        "            columns=['node_id', 'node_attr']\n",
        "        )\n",
        "\n",
        "        # Convert edge list to DataFrame\n",
        "        edges_df = pd.DataFrame(edges, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "        # Save to CSV\n",
        "        nodes_df.to_csv(f'{path_nodes}/{i}.csv', index=False)\n",
        "        edges_df.to_csv(f'{path_edges}/{i}.csv', index=False)\n",
        "\n",
        "preprocessing_step_one()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15061d64-32f8-4bad-94f8-c3d630c8119f",
      "metadata": {
        "id": "15061d64-32f8-4bad-94f8-c3d630c8119f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def preprocessing_step_two():\n",
        "    print(\"Loading local knowledge base...\")\n",
        "    with open('graphs.json', 'r') as f:\n",
        "        graphs = json.load(f)\n",
        "\n",
        "    model, tokenizer, device = load_model[model_name]()\n",
        "    text2embedding = load_text2embedding[model_name]\n",
        "\n",
        "    print(\"Embedding and storing graphs in milvusDB...\")\n",
        "    os.makedirs(path_graphs, exist_ok=True)\n",
        "\n",
        "    milvus_vectors = []\n",
        "\n",
        "    for index in tqdm(range(len(graphs))):\n",
        "        # --- Load nodes & edges ---\n",
        "        nodes_path = f'{path_nodes}/{index}.csv'\n",
        "        edges_path = f'{path_edges}/{index}.csv'\n",
        "        if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
        "            print(f'Skipping graph {index} (missing files)')\n",
        "            continue\n",
        "\n",
        "        nodes = pd.read_csv(nodes_path)\n",
        "        edges = pd.read_csv(edges_path)\n",
        "        nodes.node_attr.fillna(\"\", inplace=True)\n",
        "\n",
        "        if len(nodes) == 0:\n",
        "            print(f'Empty graph at index {index}')\n",
        "            continue\n",
        "\n",
        "        # --- Embed node and edge attributes ---\n",
        "        x = text2embedding(model, tokenizer, device, nodes.node_attr.tolist())\n",
        "        edge_attr = text2embedding(model, tokenizer, device, edges.edge_attr.tolist())\n",
        "        edge_index = torch.LongTensor([edges.src.tolist(), edges.dst.tolist()])\n",
        "\n",
        "        # --- Save graph as torch_geometric.Data ---\n",
        "        pyg_graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(nodes))\n",
        "        torch.save(pyg_graph, f'{path_graphs}/{index}.pt')\n",
        "\n",
        "        # --- Compute graph-level embedding (mean of node embeddings) ---\n",
        "        graph_embedding = torch.mean(x, dim=0).cpu().tolist()\n",
        "\n",
        "        # --- Store in Milvus format: [graph_id, embedding, index] ---\n",
        "        milvus_vectors.append({\"graph_id\": index, \"embedding\": graph_embedding, \"graph_idx\": index})\n",
        "\n",
        "    # --- Final batch insert into Milvus ---\n",
        "    if milvus_vectors:\n",
        "        milvus_client.insert(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            data=milvus_vectors,\n",
        "            auto_id=False\n",
        "        )\n",
        "        milvus_client.flush(COLLECTION_NAME)\n",
        "        print(f\"Inserted {len(milvus_vectors)} graph embeddings into Milvus.\")\n",
        "    else:\n",
        "        print(\"No graphs were inserted into Milvus.\")\n",
        "\n",
        "preprocessing_step_two()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5a77b0f7-b38a-44c6-b5d1-7ee7c3f97acc",
      "metadata": {
        "id": "5a77b0f7-b38a-44c6-b5d1-7ee7c3f97acc"
      },
      "outputs": [],
      "source": [
        "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
        "    c = 0.01\n",
        "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
        "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "        graph = Data(x=graph.x, edge_index=graph.edge_index, edge_attr=graph.edge_attr, num_nodes=graph.num_nodes)\n",
        "        return graph, desc\n",
        "\n",
        "    root = -1  # unrooted\n",
        "    num_clusters = 1\n",
        "    pruning = 'gw'\n",
        "    verbosity_level = 0\n",
        "    if topk > 0:\n",
        "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.x)\n",
        "        topk = min(topk, graph.num_nodes)\n",
        "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
        "\n",
        "        n_prizes = torch.zeros_like(n_prizes)\n",
        "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
        "    else:\n",
        "        n_prizes = torch.zeros(graph.num_nodes)\n",
        "\n",
        "    if topk_e > 0:\n",
        "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.edge_attr)\n",
        "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
        "\n",
        "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
        "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
        "        last_topk_e_value = topk_e\n",
        "        for k in range(topk_e):\n",
        "            indices = e_prizes == topk_e_values[k]\n",
        "            value = min((topk_e-k)/sum(indices), last_topk_e_value)\n",
        "            e_prizes[indices] = value\n",
        "            last_topk_e_value = value*(1-c)\n",
        "        # reduce the cost of the edges such that at least one edge is selected\n",
        "        cost_e = min(cost_e, e_prizes.max().item()*(1-c/2))\n",
        "    else:\n",
        "        e_prizes = torch.zeros(graph.num_edges)\n",
        "\n",
        "    costs = []\n",
        "    edges = []\n",
        "    vritual_n_prizes = []\n",
        "    virtual_edges = []\n",
        "    virtual_costs = []\n",
        "    mapping_n = {}\n",
        "    mapping_e = {}\n",
        "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
        "        prize_e = e_prizes[i]\n",
        "        if prize_e <= cost_e:\n",
        "            mapping_e[len(edges)] = i\n",
        "            edges.append((src, dst))\n",
        "            costs.append(cost_e - prize_e)\n",
        "        else:\n",
        "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
        "            mapping_n[virtual_node_id] = i\n",
        "            virtual_edges.append((src, virtual_node_id))\n",
        "            virtual_edges.append((virtual_node_id, dst))\n",
        "            virtual_costs.append(0)\n",
        "            virtual_costs.append(0)\n",
        "            vritual_n_prizes.append(prize_e - cost_e)\n",
        "\n",
        "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
        "    num_edges = len(edges)\n",
        "    if len(virtual_costs) > 0:\n",
        "        costs = np.array(costs+virtual_costs)\n",
        "        edges = np.array(edges+virtual_edges)\n",
        "\n",
        "    vertices, edges = pcst_fast(edges, prizes, costs, root, num_clusters, pruning, verbosity_level)\n",
        "\n",
        "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
        "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
        "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "    if len(virtual_vertices) > 0:\n",
        "        virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
        "        selected_edges = np.array(selected_edges+virtual_edges)\n",
        "\n",
        "    edge_index = graph.edge_index[:, selected_edges]\n",
        "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    n = textual_nodes.iloc[selected_nodes]\n",
        "    e = textual_edges.iloc[selected_edges]\n",
        "    desc = n.to_csv(index=False)+'\\n'+e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
        "\n",
        "    x = graph.x[selected_nodes]\n",
        "    edge_attr = graph.edge_attr[selected_edges]\n",
        "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
        "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
        "    edge_index = torch.LongTensor([src, dst])\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
        "\n",
        "    return data, desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4968bf61-7aeb-4585-b812-f40aeb007800",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4968bf61-7aeb-4585-b812-f40aeb007800",
        "outputId": "281efc95-23cf-4fae-ba9d-e82a1e3a8650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving subgraphs: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], [])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "def retreival(question, k=3):\n",
        "    model, tokenizer, device = load_model[model_name]()\n",
        "    text2embedding = load_text2embedding[model_name]\n",
        "    # Encode question\n",
        "    q_emb = text2embedding(model, tokenizer, device, [question])[0]\n",
        "\n",
        "    # Ensure collection is loaded before search\n",
        "    try:\n",
        "        milvus_client.load_collection(COLLECTION_NAME)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading collection: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    # Perform similarity search in Milvus\n",
        "    search_results = milvus_client.search(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        data=[q_emb.tolist()],\n",
        "        limit=k,\n",
        "        search_params={\"metric_type\": METRIC_TYPE, \"params\": {}},\n",
        "        output_fields=[\"graph_idx\"]\n",
        "    )\n",
        "\n",
        "    # Extract graph indices from results\n",
        "    hits = search_results[0]\n",
        "    graph_indices = [hit[\"entity\"][\"graph_idx\"] for hit in hits]\n",
        "\n",
        "    # Collect subgraphs and descriptions\n",
        "    sub_graphs = []\n",
        "    descriptions = []\n",
        "\n",
        "    for index in tqdm(graph_indices, desc=\"Retrieving subgraphs\"):\n",
        "        nodes_path = f'{path_nodes}/{index}.csv'\n",
        "        edges_path = f'{path_edges}/{index}.csv'\n",
        "        graph_path = f'{path_graphs}/{index}.pt'\n",
        "\n",
        "        if not (os.path.exists(nodes_path) and os.path.exists(edges_path) and os.path.exists(graph_path)):\n",
        "            print(f\"Missing data for graph {index}\")\n",
        "            continue\n",
        "\n",
        "        nodes = pd.read_csv(nodes_path)\n",
        "        edges = pd.read_csv(edges_path)\n",
        "        if len(nodes) == 0:\n",
        "            print(f\"Empty graph at index {index}\")\n",
        "            continue\n",
        "\n",
        "        graph = torch.load(graph_path)\n",
        "\n",
        "        # Apply your custom retrieval logic (must be defined elsewhere)\n",
        "        subg, desc = retrieval_via_pcst(\n",
        "            graph=graph,\n",
        "            q_emb=q_emb,\n",
        "            textual_nodes=nodes,\n",
        "            textual_edges=edges,\n",
        "            topk=3,\n",
        "            topk_e=5,\n",
        "            cost_e=0.5\n",
        "        )\n",
        "\n",
        "        sub_graphs.append(subg)\n",
        "        descriptions.append(desc)\n",
        "\n",
        "    return sub_graphs, descriptions\n",
        "\n",
        "question = \"What are the common symptoms of asthma?\"\n",
        "retreival(question, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def construct_textualize_prompt(description, subgraphs):\n",
        "    \"\"\"\n",
        "    Constructs a prompt for the textualization model using the knowledge graph description.\n",
        "    \"\"\"\n",
        "    # Start with a system message or instruction\n",
        "    system_message = \"\"\"You are a helpful assistant that summarizes medical knowledge graphs into coherent text.\n",
        "Based on the provided knowledge graph nodes and relationships, generate a concise and informative paragraph describing the medical concepts and their connections.\n",
        "Do not include node IDs or graph numbers in your summary. Be very concise and stick to the subgraphs and descriptions provided. Don't add any new information\n",
        "\"\"\"\n",
        "\n",
        "    # Format the context from the knowledge graph description\n",
        "    # Split the description into nodes and edges\n",
        "    nodes = description[0]\n",
        "    edges = description[1] if len(description) > 1 else \"\"\n",
        "\n",
        "    context = \"Here is a medical knowledge graph description:\\n\"\n",
        "    context += f\"Nodes:\\n{nodes}\\n\"\n",
        "    if edges:\n",
        "        context += f\"Relationships:\\n{edges}\\n\"\n",
        "\n",
        "    # Construct the final prompt\n",
        "    prompt = f\"{system_message}\\n\\n{context}\\nGenerate a summary of this knowledge graph:\\n\\nSummary:\"\n",
        "    print(\"textualize prompt:\", prompt)\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "5eAOYk4vU_KJ"
      },
      "id": "5eAOYk4vU_KJ",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI or NVIDIA client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "JX5L02syW248"
      },
      "id": "JX5L02syW248",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "b2ce069c-2e34-4697-a0d9-392144ddb798",
      "metadata": {
        "id": "b2ce069c-2e34-4697-a0d9-392144ddb798"
      },
      "outputs": [],
      "source": [
        "def construct_rag_prompt(question, context):\n",
        "    \"\"\"\n",
        "    Constructs a prompt for RAG using the textualized knowledge graph descriptions.\n",
        "    \"\"\"\n",
        "    # Start with system message\n",
        "    system_message = \"\"\"You are a medical knowledge assistant that helps answer questions based on medical knowledge graphs.\n",
        "Use only the provided medical context derived from knowledge graphs to answer the question. If the provided context doesn't contain enough information to fully answer the question,\n",
        "say I don't know. If the context doesn't contain any relevant information, say I can't answer this question. Otherwise, if the context has enough information don't use the phrases I don't know and I can't answer this question.\n",
        "Be precise and factual in your responses. Format the responses for more clarity.\n",
        "\"\"\"\n",
        "\n",
        "    # Format the context from textualized descriptions\n",
        "    context = f\"Here is the relevant medical context derived from knowledge graphs:\\n{context}\"\n",
        "\n",
        "    # Construct the final prompt\n",
        "    prompt = f\"{system_message}\\n\\n{context}\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def textualize(descriptions,sub_graphs):\n",
        "    \"\"\"\n",
        "    Textualizes the context from sub_graphs and descriptions using an LLM API call.\n",
        "\n",
        "    Args:\n",
        "        sub_graphs (list): A list of torch_geometric.data.Data objects representing the subgraphs.\n",
        "        descriptions (list): A list of string descriptions for the subgraphs.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the LLM API call.\n",
        "    \"\"\"\n",
        "    # Construct the prompt for the LLM\n",
        "    prompt = construct_textualize_prompt(descriptions, sub_graphs)\n",
        "\n",
        "    try:\n",
        "        # Make the API call to the LLM\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=\"mistralai/mixtral-8x7b-instruct-v0.1\"\n",
        "        )\n",
        "\n",
        "        # Extract the response\n",
        "        llm_response = chat_completion.choices[0].message.content\n",
        "        print(f\"\\n\\ncontext: \",llm_response)\n",
        "        return llm_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM API call: {e}\")\n",
        "        return \"An error occurred while generating the response.\""
      ],
      "metadata": {
        "id": "v61ieKfPUkX4"
      },
      "id": "v61ieKfPUkX4",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(question, context, response, label=None, report=False):\n",
        "    \"\"\"\n",
        "    Evaluates the LLM's response based on clarity, exactitude, and context adherence.\n",
        "\n",
        "    Args:\n",
        "        question (str): Original question\n",
        "        context (str): Retrieved knowledge graph context\n",
        "        response (str): LLM's response to evaluate\n",
        "        label (str, optional): Ground truth label for comparison\n",
        "        report (bool): Whether to save evaluation to file\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation scores and feedback\n",
        "    \"\"\"\n",
        "\n",
        "    if report:\n",
        "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        with open(f\"evaluation_{current_date}.txt\", \"a\") as f:\n",
        "            f.write(f\"Question: {question}\\n\")\n",
        "            f.write(f\"Context: {context}\\n\")\n",
        "            f.write(f\"Response: {response}\\n\")\n",
        "            f.write(f\"Label: {label}\\n\")\n",
        "\n",
        "\n",
        "    evaluation_prompt = f\"\"\"You are an expert evaluator of medical knowledge responses. Evaluate the following response based on three criteria:\n",
        "\n",
        "1. Clarity (0-5): How clear and well-structured is the response? 0 is the worst, 1 is the best.\n",
        "2. Exactitude (0-5): How accurate and precise is the information provided? 0 is the worst, 1 is the best.\n",
        "3. Context Adherence (0-5): How well does the response stick to the provided knowledge graphs? 0 is the worst, 1 is the best.\n",
        "4. Relevance (0-5): How relevant is the retrieved Knowledge Graph Context to the question? 0 is the worst, 1 is the best.\n",
        "5. Completeness (0-5): How complete and thorough is the response? 0 is the worst, 1 is the best.\n",
        "6. Logical Flow (0-5): How coherent and well-structured is the response? 0 is the worst, 1 is the best.\n",
        "7. Uncertainty Handling (0-5): How well does the response acknowledge limitations and uncertainties? 0 is the worst, 1 is the best.\n",
        "\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Knowledge Graph Context:\n",
        "{context}\n",
        "\n",
        "Response to Evaluate:\n",
        "{response}\n",
        "\n",
        "Provide your evaluation in the following format:\n",
        "CLARITY: [score]/5 - [brief explanation]\n",
        "EXACTITUDE: [score]/5 - [brief explanation]\n",
        "CONTEXT ADHERENCE: [score]/5 - [brief explanation]\n",
        "RELEVANCE: [score]/5 - [brief explanation]\n",
        "COMPLETENESS: [score]/5 - [brief explanation]\n",
        "LOGICAL FLOW: [score]/5 - [brief explanation]\n",
        "UNCERTAINTY HANDLING: [score]/5 - [brief explanation]\n",
        "OVERALL FEEDBACK: [average score] and 2-3 sentences summarizing the evaluation]\n",
        "\"\"\"\n",
        "\n",
        "    if label is not None:\n",
        "        evaluation_prompt += f\"Ground Truth Label: {label}\\n\"\n",
        "\n",
        "    # new client (llm api) for evaluation different  meta/llama-3.1-405b-instruct\n",
        "\n",
        "    evaluation = client.chat.completions.create(\n",
        "        model=\"meta/llama-3.1-405b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert evaluator of medical knowledge responses.\"},\n",
        "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "        ],\n",
        "        temperature=0.2,  # Lower temperature for more consistent evaluation\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # print(\"\\n=== Response Evaluation ===\\n\")\n",
        "\n",
        "    # Initialize variables to store the complete response\n",
        "    full_evaluation = \"\"\n",
        "    scores = {\n",
        "        \"clarity_score\": None,\n",
        "        \"exactitude_score\": None,\n",
        "        \"context_adherence_score\": None,\n",
        "        \"relevance_score\": None,\n",
        "        \"completeness_score\": None,\n",
        "        \"logical_flow_score\": None,\n",
        "        \"uncertainty_handling_score\": None,\n",
        "        \"overall_feedback\": None\n",
        "    }\n",
        "\n",
        "    for chunk in evaluation:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            # print(content, end=\"\")\n",
        "            full_evaluation += content\n",
        "\n",
        "    # Extract scores from the complete evaluation\n",
        "    scores[\"clarity_score\"] = re.findall(r\"CLARITY: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"exactitude_score\"] = re.findall(r\"EXACTITUDE: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"context_adherence_score\"] = re.findall(r\"CONTEXT ADHERENCE: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"relevance_score\"] = re.findall(r\"RELEVANCE: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"completeness_score\"] = re.findall(r\"COMPLETENESS: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"logical_flow_score\"] = re.findall(r\"LOGICAL FLOW: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"uncertainty_handling_score\"] = re.findall(r\"UNCERTAINTY HANDLING: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "    scores[\"overall_feedback\"] = re.findall(r\"OVERALL FEEDBACK: (\\d+(?:\\.\\d+)?)\", full_evaluation)\n",
        "\n",
        "    # if the response is \"I don't know\" set the scores\n",
        "    if \"I can't answer this question\" in response:\n",
        "        scores[\"clarity_score\"] = 0\n",
        "        scores[\"exactitude_score\"] = 0\n",
        "        scores[\"context_adherence_score\"] = 0\n",
        "        scores[\"relevance_score\"] = 0\n",
        "        scores[\"completeness_score\"] = 0\n",
        "        scores[\"logical_flow_score\"] = 0\n",
        "        scores[\"uncertainty_handling_score\"] = 0\n",
        "        scores[\"overall_feedback\"] = 0\n",
        "\n",
        "    # Convert list matches to single values\n",
        "    for key in scores:\n",
        "        if scores[key]:\n",
        "            scores[key] = scores[key][0] if isinstance(scores[key], list) else scores[key]\n",
        "\n",
        "    # print(\"\\n=== Evaluation Complete ===\\n\")\n",
        "\n",
        "    if report:\n",
        "        with open(f\"evaluation_{current_date}.txt\", \"a\") as f:\n",
        "            f.write(\"\\nScores:\\n\")\n",
        "            f.write(json.dumps(scores, indent=2))\n",
        "            f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "KROE_WPtpemZ",
        "collapsed": true
      },
      "id": "KROE_WPtpemZ",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question):\n",
        "  sub_graphs, descriptions = retreival(question, k=3)\n",
        "  context = textualize(descriptions, sub_graphs)\n",
        "  rag_prompt = construct_rag_prompt(question, context, sub_graphs) # the descriptions are textualized in the construct_rag_prompt function\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"nvidia/llama-3.1-nemotron-ultra-253b-v1\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a medical knowledge assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": rag_prompt}\n",
        "      ],\n",
        "      temperature=0.4,\n",
        "      top_p=0.95,\n",
        "      max_tokens=4096,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stream=True\n",
        "  )\n",
        "\n",
        "  #   words to delete from the response:\n",
        "  to_delete = [\n",
        "    r\"\\(Knowledge Graph \\d+\\)\",\n",
        "    r\"Knowledge Graph \\d+\"\n",
        "  ]\n",
        "\n",
        "  # Print the streaming response and save the response\n",
        "  full_response = \"\"\n",
        "  for chunk in completion:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "      content = chunk.choices[0].delta.content\n",
        "      for word in to_delete:\n",
        "          content = re.sub(word, \"\", content)\n",
        "      # print(content, end=\"\")\n",
        "      full_response += content\n",
        "\n",
        "  scores = evaluate_response(question, descriptions, full_response)\n",
        "  return full_response, scores\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "questions = [\n",
        "    \"How does air pollution impact the treatment or worsening of asthma and COPD symptoms?\",\n",
        "    \"How does air pollution impact the treatment or worsening of COPD symptoms?\",\n",
        "    \"How does air pollution impact the treatment or worsening of asthma symptoms?\",\n",
        "    \"What does asthma mean?\",\n",
        "    \"What is the color of Zied's shoes?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n\\nQuestion: {question}\\n\")\n",
        "    # Get the response\n",
        "    full_response, score = generate_response(question)\n",
        "    print(f\"Response: {full_response}\\n\")\n",
        "    print(f\"Score: {score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "QE1wN3V9un0y",
        "outputId": "d8e5c6f0-526a-43f5-f6d7-8c0ce1d47f58",
        "collapsed": true
      },
      "id": "QE1wN3V9un0y",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Question: How does air pollution impact the treatment or worsening of asthma and COPD symptoms?\n",
            "\n",
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving subgraphs: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-0d150313b189>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\nQuestion: {question}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mfull_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response: {full_response}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Score: {score}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-0d150313b189>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msub_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretreival\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrag_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_rag_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_graphs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the descriptions are textualized in the construct_rag_prompt function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-3afa7a974df3>\u001b[0m in \u001b[0;36mtextualize\u001b[0;34m(descriptions, sub_graphs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Construct the prompt for the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_textualize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-a851bfc7492a>\u001b[0m in \u001b[0;36mconstruct_textualize_prompt\u001b[0;34m(description, subgraphs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Format the context from the knowledge graph description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Split the description into nodes and edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking"
      ],
      "metadata": {
        "id": "WDvgVcjXb8Xp"
      },
      "id": "WDvgVcjXb8Xp"
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.load_qa_data import load_qa\n",
        "\n",
        "all_questions, all_answers= load_qa()\n",
        "# all_questions"
      ],
      "metadata": {
        "id": "d4OGBk_F5FMV",
        "outputId": "9cb28e25-02db-47b7-83de-47b3e3bd4344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d4OGBk_F5FMV",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading file  generated_qa/all_covid_data_qa.json\n",
            "loading file  generated_qa/all_asthma_data_qa.json\n",
            "loading file  generated_qa/all_pneumonia_data_qa.json\n",
            "loading file  generated_qa/all_copcd_data_qa.json\n",
            "loading file  generated_qa/all_tuberculosis_data_qa.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_score = 0\n",
        "for i, question in enumerate(all_questions):\n",
        "    response = generate_response(question)\n",
        "    sub_graphs, descriptions = retreival(question, k=3)\n",
        "    scores = evaluate_response(question, descriptions, response, all_answers[i], True )\n",
        "    sum_score += float(scores[\"overall_feedback\"])\n",
        "    # print(sum_score)\n",
        "average = sum_score / len(all_questions)\n",
        "print(average)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mw9tctRj5MD9",
        "outputId": "67cd038e-7cda-4d87-9ecd-e8de7db7ae00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "id": "mw9tctRj5MD9",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving subgraphs: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving subgraphs: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6dfd698f6070>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msub_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretreival\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msum_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"overall_feedback\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ce0d0e53541c>\u001b[0m in \u001b[0;36mtextualize\u001b[0;34m(sub_graphs, descriptions)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Construct the prompt for the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_textualize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6f426cae44b3>\u001b[0m in \u001b[0;36mconstruct_textualize_prompt\u001b[0;34m(description)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Format the context from the knowledge graph description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Split the description into nodes and edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pfa env",
      "language": "python",
      "name": "pfacuda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89211a-d258-440e-b6a3-0c16234e99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc4b816-97ef-4910-8f6b-83c67f19dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22143a813a9a4182bf76984e34c1287f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import json\n",
    "import gensim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from pcst_fast import pcst_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcddaf-1b0b-4e77-ae31-61d190f01748",
   "metadata": {},
   "source": [
    "# Load embedding modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "893d4130-c2ff-4c0b-b48c-768307a4bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_repo = 'sentence-transformers/all-roberta-large-v1'\n",
    "batch_size = 1024  # Adjust the batch size as needed\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids=None, attention_mask=None):\n",
    "        super().__init__()\n",
    "        self.data = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"att_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"input_ids\"].size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = index.item()\n",
    "        batch_data = dict()\n",
    "        for key in self.data.keys():\n",
    "            if self.data[key] is not None:\n",
    "                batch_data[key] = self.data[key][index]\n",
    "        return batch_data\n",
    "        \n",
    "class Sentence_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_repo):\n",
    "        super(Sentence_Transformer, self).__init__()\n",
    "        print(f\"inherit model weights from {pretrained_repo}\")\n",
    "        self.bert_model = AutoModel.from_pretrained(pretrained_repo)\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        data_type = token_embeddings.dtype\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).to(data_type)\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, input_ids, att_mask):\n",
    "        bert_out = self.bert_model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        sentence_embeddings = self.mean_pooling(bert_out, att_mask)\n",
    "\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings\n",
    "\n",
    "def load_sbert():\n",
    "\n",
    "    model = Sentence_Transformer(pretrained_repo)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_repo)\n",
    "\n",
    "    # data parallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def sbert_text2embedding(model, tokenizer, device, text):\n",
    "    if len(text) == 0:\n",
    "        return torch.zeros((0, 1024))\n",
    "\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    dataset = Dataset(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Placeholder for storing the embeddings\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Iterate through batches\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Move batch to the appropriate device\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings = model(input_ids=batch[\"input_ids\"], att_mask=batch[\"att_mask\"])\n",
    "\n",
    "            # Append the embeddings to the list\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    # Concatenate the embeddings from all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0).cpu()\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "load_model = {\n",
    "    'sbert': load_sbert,\n",
    "}\n",
    "\n",
    "load_text2embedding = {\n",
    "    'sbert': sbert_text2embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187b1d0-b39d-4639-bfca-fbfc503a1245",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7415b6-98ed-4e43-adf8-d912f36abf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sbert'\n",
    "path = 'dataset'\n",
    "path_nodes = f'{path}/nodes'\n",
    "path_edges = f'{path}/edges'\n",
    "path_graphs = f'{path}/graphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e89fb80-6df3-4c6d-8db4-5964d11915f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType, FieldSchema, CollectionSchema\n",
    "\n",
    "CLUSTER_ENDPOINT = (\n",
    "    \"https://in03-7a5f9d2a1aa84ef.serverless.gcp-us-west1.cloud.zilliz.com\"\n",
    ")\n",
    "API_KEY = (\n",
    "    \"a73c79fb1924d05aeb410abc0d5669293cc33be37a123953be640725aa42198ef5c1e499cc07f231977c742ad6e6977c6eddec05\"\n",
    ")\n",
    "\n",
    "milvus_client = MilvusClient(uri=CLUSTER_ENDPOINT, token=API_KEY)\n",
    "\n",
    "COLLECTION_NAME = \"graph_embeddings\"\n",
    "VECTOR_DIM      = 1024          # SBERT default; change if yours differs\n",
    "METRIC_TYPE     = \"COSINE\"     # or \"IP\" for inner-product\n",
    "\n",
    "# Drop the collection if it already exists (for a clean slate)\n",
    "if COLLECTION_NAME in milvus_client.list_collections():\n",
    "    milvus_client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Define the schema\n",
    "schema = CollectionSchema(\n",
    "    fields=[\n",
    "        FieldSchema(\n",
    "            name=\"graph_id\",\n",
    "            dtype=DataType.INT64,\n",
    "            is_primary=True,\n",
    "            auto_id=False\n",
    "        ),\n",
    "        FieldSchema(\n",
    "            name=\"embedding\",\n",
    "            dtype=DataType.FLOAT_VECTOR,\n",
    "            dim=VECTOR_DIM\n",
    "        ),\n",
    "        FieldSchema(                 # optional metadata field\n",
    "            name=\"graph_idx\",\n",
    "            dtype=DataType.INT64\n",
    "        )\n",
    "    ],\n",
    "    description=\"Mean-pooled SBERT embeddings of knowledge graphs\"\n",
    ")\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "index_params.add_index(\"embedding\", index_type=\"IVF_FLAT\", metric_type=\"COSINE\", index_params={\"nlist\": 64})\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    consistency_level=\"Strong\",\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c868ece-0fe5-4197-88f8-c5cdfad18fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 312.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_step_one():\n",
    "    # Load the graphs from the JSON file\n",
    "    with open('graphs.json', 'r') as f:\n",
    "        graphs = json.load(f)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(path_nodes, exist_ok=True)\n",
    "    os.makedirs(path_edges, exist_ok=True)\n",
    "\n",
    "    # Process each graph\n",
    "    for i, triples in enumerate(tqdm(graphs)):\n",
    "        node_map = {}   # Maps node label → node ID\n",
    "        edges = []\n",
    "    \n",
    "        for h, r, t in triples:\n",
    "            h = h.lower()\n",
    "            t = t.lower()\n",
    "            if h not in node_map:\n",
    "                node_map[h] = len(node_map)\n",
    "            if t not in node_map:\n",
    "                node_map[t] = len(node_map)\n",
    "            edges.append({'src': node_map[h], 'edge_attr': r, 'dst': node_map[t]})\n",
    "    \n",
    "        # Convert node map to DataFrame\n",
    "        nodes_df = pd.DataFrame(\n",
    "            [{'node_id': v, 'node_attr': k} for k, v in node_map.items()],\n",
    "            columns=['node_id', 'node_attr']\n",
    "        )\n",
    "    \n",
    "        # Convert edge list to DataFrame\n",
    "        edges_df = pd.DataFrame(edges, columns=['src', 'edge_attr', 'dst'])\n",
    "    \n",
    "        # Save to CSV\n",
    "        nodes_df.to_csv(f'{path_nodes}/{i}.csv', index=False)\n",
    "        edges_df.to_csv(f'{path_edges}/{i}.csv', index=False)\n",
    "\n",
    "preprocessing_step_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15061d64-32f8-4bad-94f8-c3d630c8119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local knowledge base...\n",
      "inherit model weights from sentence-transformers/all-roberta-large-v1\n",
      "Embedding and storing graphs in milvusDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_4376\\2052477268.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nodes.node_attr.fillna(\"\", inplace=True)\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:00<00:01,  2.58it/s]C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_4376\\2052477268.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nodes.node_attr.fillna(\"\", inplace=True)\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:00<00:01,  2.22it/s]C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_4376\\2052477268.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nodes.node_attr.fillna(\"\", inplace=True)\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:01<00:01,  1.79it/s]C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_4376\\2052477268.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nodes.node_attr.fillna(\"\", inplace=True)\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:02<00:00,  1.77it/s]C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_4376\\2052477268.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nodes.node_attr.fillna(\"\", inplace=True)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 5 graph embeddings into Milvus.\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_step_two():\n",
    "    print(\"Loading local knowledge base...\")\n",
    "    with open('graphs.json', 'r') as f:\n",
    "        graphs = json.load(f)\n",
    "\n",
    "    model, tokenizer, device = load_model[model_name]()\n",
    "    text2embedding = load_text2embedding[model_name]\n",
    "\n",
    "    print(\"Embedding and storing graphs in milvusDB...\")\n",
    "    os.makedirs(path_graphs, exist_ok=True)\n",
    "\n",
    "    milvus_vectors = []  \n",
    "\n",
    "    for index in tqdm(range(len(graphs))):\n",
    "        # --- Load nodes & edges ---\n",
    "        nodes_path = f'{path_nodes}/{index}.csv'\n",
    "        edges_path = f'{path_edges}/{index}.csv'\n",
    "        if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "            print(f'Skipping graph {index} (missing files)')\n",
    "            continue\n",
    "\n",
    "        nodes = pd.read_csv(nodes_path)\n",
    "        edges = pd.read_csv(edges_path)\n",
    "        nodes.node_attr.fillna(\"\", inplace=True)\n",
    "\n",
    "        if len(nodes) == 0:\n",
    "            print(f'Empty graph at index {index}')\n",
    "            continue\n",
    "\n",
    "        # --- Embed node and edge attributes ---\n",
    "        x = text2embedding(model, tokenizer, device, nodes.node_attr.tolist())\n",
    "        edge_attr = text2embedding(model, tokenizer, device, edges.edge_attr.tolist())\n",
    "        edge_index = torch.LongTensor([edges.src.tolist(), edges.dst.tolist()])\n",
    "\n",
    "        # --- Save graph as torch_geometric.Data ---\n",
    "        pyg_graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(nodes))\n",
    "        torch.save(pyg_graph, f'{path_graphs}/{index}.pt')\n",
    "\n",
    "        # --- Compute graph-level embedding (mean of node embeddings) ---\n",
    "        graph_embedding = torch.mean(x, dim=0).cpu().tolist()\n",
    "\n",
    "        # --- Store in Milvus format: [graph_id, embedding, index] ---\n",
    "        milvus_vectors.append({\"graph_id\": index, \"embedding\": graph_embedding, \"graph_idx\": index})\n",
    "\n",
    "    # --- Final batch insert into Milvus ---\n",
    "    if milvus_vectors:\n",
    "        milvus_client.insert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            data=milvus_vectors,\n",
    "            auto_id=False\n",
    "        )\n",
    "        milvus_client.flush(COLLECTION_NAME)\n",
    "        print(f\"Inserted {len(milvus_vectors)} graph embeddings into Milvus.\")\n",
    "    else:\n",
    "        print(\"No graphs were inserted into Milvus.\")\n",
    "\n",
    "preprocessing_step_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a77b0f7-b38a-44c6-b5d1-7ee7c3f97acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
    "    c = 0.01\n",
    "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
    "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
    "        graph = Data(x=graph.x, edge_index=graph.edge_index, edge_attr=graph.edge_attr, num_nodes=graph.num_nodes)\n",
    "        return graph, desc\n",
    "\n",
    "    root = -1  # unrooted\n",
    "    num_clusters = 1\n",
    "    pruning = 'gw'\n",
    "    verbosity_level = 0\n",
    "    if topk > 0:\n",
    "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.x)\n",
    "        topk = min(topk, graph.num_nodes)\n",
    "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
    "\n",
    "        n_prizes = torch.zeros_like(n_prizes)\n",
    "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
    "    else:\n",
    "        n_prizes = torch.zeros(graph.num_nodes)\n",
    "\n",
    "    if topk_e > 0:\n",
    "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.edge_attr)\n",
    "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
    "\n",
    "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
    "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
    "        last_topk_e_value = topk_e\n",
    "        for k in range(topk_e):\n",
    "            indices = e_prizes == topk_e_values[k]\n",
    "            value = min((topk_e-k)/sum(indices), last_topk_e_value)\n",
    "            e_prizes[indices] = value\n",
    "            last_topk_e_value = value*(1-c)\n",
    "        # reduce the cost of the edges such that at least one edge is selected\n",
    "        cost_e = min(cost_e, e_prizes.max().item()*(1-c/2))\n",
    "    else:\n",
    "        e_prizes = torch.zeros(graph.num_edges)\n",
    "\n",
    "    costs = []\n",
    "    edges = []\n",
    "    vritual_n_prizes = []\n",
    "    virtual_edges = []\n",
    "    virtual_costs = []\n",
    "    mapping_n = {}\n",
    "    mapping_e = {}\n",
    "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
    "        prize_e = e_prizes[i]\n",
    "        if prize_e <= cost_e:\n",
    "            mapping_e[len(edges)] = i\n",
    "            edges.append((src, dst))\n",
    "            costs.append(cost_e - prize_e)\n",
    "        else:\n",
    "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
    "            mapping_n[virtual_node_id] = i\n",
    "            virtual_edges.append((src, virtual_node_id))\n",
    "            virtual_edges.append((virtual_node_id, dst))\n",
    "            virtual_costs.append(0)\n",
    "            virtual_costs.append(0)\n",
    "            vritual_n_prizes.append(prize_e - cost_e)\n",
    "\n",
    "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
    "    num_edges = len(edges)\n",
    "    if len(virtual_costs) > 0:\n",
    "        costs = np.array(costs+virtual_costs)\n",
    "        edges = np.array(edges+virtual_edges)\n",
    "\n",
    "    vertices, edges = pcst_fast(edges, prizes, costs, root, num_clusters, pruning, verbosity_level)\n",
    "\n",
    "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
    "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
    "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
    "    if len(virtual_vertices) > 0:\n",
    "        virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
    "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
    "        selected_edges = np.array(selected_edges+virtual_edges)\n",
    "\n",
    "    edge_index = graph.edge_index[:, selected_edges]\n",
    "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
    "\n",
    "    n = textual_nodes.iloc[selected_nodes]\n",
    "    e = textual_edges.iloc[selected_edges]\n",
    "    desc = n.to_csv(index=False)+'\\n'+e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
    "\n",
    "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
    "\n",
    "    x = graph.x[selected_nodes]\n",
    "    edge_attr = graph.edge_attr[selected_edges]\n",
    "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
    "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
    "    edge_index = torch.LongTensor([src, dst])\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
    "\n",
    "    return data, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4968bf61-7aeb-4585-b812-f40aeb007800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Retrieving subgraphs: 100%|██████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 55.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Data(x=[5, 1024], edge_index=[2, 4], edge_attr=[4, 1024], num_nodes=5),\n",
       "  Data(x=[7, 1024], edge_index=[2, 6], edge_attr=[6, 1024], num_nodes=7),\n",
       "  Data(x=[9, 1024], edge_index=[2, 8], edge_attr=[8, 1024], num_nodes=9)],\n",
       " ['node_id,node_attr\\r\\n0,air pollution\\r\\n1,asthma attacks\\r\\n2,copd\\r\\n3,pm2.5\\r\\n4,lung function\\r\\n\\nsrc,edge_attr,dst\\r\\n0,linked_to,1\\r\\n0,linked_to,2\\r\\n3,component_of,0\\r\\n3,damages,4\\r\\n',\n",
       "  'node_id,node_attr\\r\\n0,copd\\r\\n2,emphysema\\r\\n3,shortness of breath\\r\\n4,oxygen therapy\\r\\n5,smoking\\r\\n6,alveoli\\r\\n7,severe copd\\r\\n\\nsrc,edge_attr,dst\\r\\n4,used_in,7\\r\\n0,includes,2\\r\\n0,symptom,3\\r\\n0,treated_with,4\\r\\n5,risk_factor_for,0\\r\\n2,damages,6\\r\\n',\n",
       "  'node_id,node_attr\\r\\n0,asthma\\r\\n1,inhaled corticosteroids\\r\\n2,airway inflammation\\r\\n3,bronchodilators\\r\\n4,wheezing\\r\\n5,shortness of breath\\r\\n6,regular monitoring\\r\\n7,allergic asthma\\r\\n8,dust mites\\r\\n\\nsrc,edge_attr,dst\\r\\n7,triggered_by,8\\r\\n0,treated_with,1\\r\\n1,reduces,2\\r\\n0,treated_with,3\\r\\n3,relieve,4\\r\\n3,relieve,5\\r\\n0,requires,6\\r\\n8,cause,2\\r\\n'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retreival(question, k=3):\n",
    "    model, tokenizer, device = load_model[model_name]()\n",
    "    text2embedding = load_text2embedding[model_name]\n",
    "    # Encode question\n",
    "    q_emb = text2embedding(model, tokenizer, device, [question])[0]  \n",
    "\n",
    "    # Ensure collection is loaded before search\n",
    "    try:\n",
    "        milvus_client.load_collection(COLLECTION_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading collection: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    # Perform similarity search in Milvus\n",
    "    search_results = milvus_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=[q_emb.tolist()],\n",
    "        limit=k,\n",
    "        search_params={\"metric_type\": METRIC_TYPE, \"params\": {}},\n",
    "        output_fields=[\"graph_idx\"]\n",
    "    )\n",
    "\n",
    "    # Extract graph indices from results\n",
    "    hits = search_results[0]\n",
    "    graph_indices = [hit[\"entity\"][\"graph_idx\"] for hit in hits]\n",
    "\n",
    "    # Collect subgraphs and descriptions\n",
    "    sub_graphs = []\n",
    "    descriptions = []\n",
    "\n",
    "    for index in tqdm(graph_indices, desc=\"Retrieving subgraphs\"):\n",
    "        nodes_path = f'{path_nodes}/{index}.csv'\n",
    "        edges_path = f'{path_edges}/{index}.csv'\n",
    "        graph_path = f'{path_graphs}/{index}.pt'\n",
    "\n",
    "        if not (os.path.exists(nodes_path) and os.path.exists(edges_path) and os.path.exists(graph_path)):\n",
    "            print(f\"Missing data for graph {index}\")\n",
    "            continue\n",
    "\n",
    "        nodes = pd.read_csv(nodes_path)\n",
    "        edges = pd.read_csv(edges_path)\n",
    "        if len(nodes) == 0:\n",
    "            print(f\"Empty graph at index {index}\")\n",
    "            continue\n",
    "\n",
    "        graph = torch.load(graph_path)\n",
    "\n",
    "        # Apply your custom retrieval logic (must be defined elsewhere)\n",
    "        subg, desc = retrieval_via_pcst(\n",
    "            graph=graph,\n",
    "            q_emb=q_emb,\n",
    "            textual_nodes=nodes,\n",
    "            textual_edges=edges,\n",
    "            topk=3,\n",
    "            topk_e=5,\n",
    "            cost_e=0.5\n",
    "        )\n",
    "\n",
    "        sub_graphs.append(subg)\n",
    "        descriptions.append(desc)\n",
    "\n",
    "    return sub_graphs, descriptions\n",
    "\n",
    "question = \"How does air pollution impact the treatment or worsening of asthma and COPD symptoms?\"\n",
    "retreival(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce069c-2e34-4697-a0d9-392144ddb798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfa env",
   "language": "python",
   "name": "pfacuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

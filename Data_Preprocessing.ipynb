{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a2618e-829f-4bc1-b821-93d6d1852a02",
   "metadata": {},
   "source": [
    "# I. Scrapping & Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c88fb93-d5c9-4f7d-9573-05913ed9e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import fitz  \n",
    "import requests\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e2ec8d3-262a-4f4a-9b8f-99fcdbbae1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WHO_scrapping(save_path, disease, desired_sections):\n",
    "    url = \"https://www.who.int/news-room/fact-sheets/detail/\" + disease\n",
    "    file_name = \"who_0.json\"\n",
    "    file_path = Path(save_path) / file_name\n",
    "\n",
    "    try:\n",
    "        if file_path.exists():\n",
    "            print(f\"Already exists, skipping: {file_name}\")\n",
    "            return\n",
    "\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract specific h2 sections\n",
    "        article = soup.find(\"section\", {\"id\": \"content\"})\n",
    "        extracted_sections = {}\n",
    "\n",
    "        if article:\n",
    "            headers = article.find_all(\"h2\")\n",
    "            for h2 in headers:\n",
    "                title = h2.get_text(strip=True).lower()\n",
    "                for key in desired_sections:\n",
    "                    if key in title:\n",
    "                        content = []\n",
    "                        for sibling in h2.find_next_siblings():\n",
    "                            if sibling.name == \"h2\":\n",
    "                                break\n",
    "                            # Handle <p> and <li> tags with proper formatting\n",
    "                            if sibling.name == \"p\":\n",
    "                                text = sibling.get_text(separator=\" \", strip=True)\n",
    "                                content.append(text)\n",
    "                            elif sibling.name == \"ul\":\n",
    "                                items = sibling.find_all(\"li\")\n",
    "                                for item in items:\n",
    "                                    item_text = item.get_text(separator=\" \", strip=True)\n",
    "                                    content.append(item_text + \",\")\n",
    "                        combined = \" \".join(content)\n",
    "                        # Clean up extra whitespace and Unicode chars\n",
    "                        cleaned = (\n",
    "                            combined.replace(\"\\t\", \" \")\n",
    "                            .replace(\"\\xa0\", \" \")\n",
    "                            .replace(\" ,\", \",\")\n",
    "                            .replace(\"  \", \"\")\n",
    "                            .replace(\".,\", \",\")\n",
    "                            .strip()\n",
    "                        )\n",
    "                        extracted_sections[key.capitalize()] = cleaned\n",
    "                        break\n",
    "\n",
    "        extracted_sections[\"url\"] = url\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(extracted_sections, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Scraped and saved: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or parse: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a67268da-4f18-44bd-b649-3b098668d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDC_scrapping(save_path, disease, desired_pages):\n",
    "    base_url = \"https://www.cdc.gov/\" + disease\n",
    "    id = 0\n",
    "\n",
    "    for sub_path, sections in desired_pages.items():\n",
    "        url = base_url + sub_path\n",
    "        file_name = f\"cdc_{id}.json\"\n",
    "        file_path = Path(save_path) / file_name\n",
    "\n",
    "        try:\n",
    "            if file_path.exists():\n",
    "                print(f\"Already exists, skipping: {file_name}\")\n",
    "                id += 1\n",
    "                continue\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            extracted_sections = {}\n",
    "\n",
    "            # All <div class=\"dfe-section\">\n",
    "            all_sections = soup.find_all(\"div\", class_=\"dfe-section\")\n",
    "\n",
    "            for div in all_sections:\n",
    "                h2 = div.find(\"h2\")\n",
    "                if not h2:\n",
    "                    continue\n",
    "\n",
    "                title = h2.get_text(strip=True).lower()\n",
    "\n",
    "                for desired in sections:\n",
    "                    if desired.lower() in title:\n",
    "                        content_parts = []\n",
    "\n",
    "                        # Include h3s, ps, and lis\n",
    "                        for tag in div.find_all([\"h3\", \"p\", \"li\"]):\n",
    "                            txt = tag.get_text(separator=\" \", strip=True)\n",
    "                            if txt:\n",
    "                                content_parts.append(txt)\n",
    "\n",
    "                        # Join and clean\n",
    "                        combined = \", \".join(content_parts)\n",
    "                        cleaned = (\n",
    "                            combined.replace(\"\\t\", \" \")\n",
    "                            .replace(\"\\xa0\", \" \")\n",
    "                            .replace(\" .\", \".\")\n",
    "                            .replace(\" ,\", \",\")\n",
    "                            .replace(\"  \", \" \")\n",
    "                            .replace(\".,\", \".\")\n",
    "                            .replace(\":,\", \":\")\n",
    "                            .replace(\"\\\"\", \"\")\n",
    "                            .replace(\"”\", \"\")\n",
    "                            .replace(\"“\", \"\")\n",
    "                            .strip()\n",
    "                        )\n",
    "\n",
    "                        extracted_sections[desired] = cleaned\n",
    "                        break\n",
    "\n",
    "            extracted_sections[\"url\"] = url\n",
    "            # Save to JSON\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(extracted_sections, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Scraped and saved: {file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download or parse {url}: {e}\")\n",
    "        \n",
    "        id += 1  # Always increment after processing one URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "223fa9dc-df61-4e8e-a355-7e0bfcf70465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIH_scrapping(save_path, disease, desired_pages):\n",
    "    base_url = \"https://www.nhlbi.nih.gov/health/\" + disease\n",
    "    id = 0\n",
    "\n",
    "    for sub_path in desired_pages:\n",
    "        url = base_url + sub_path\n",
    "        file_name = f\"nih_{id}.json\"\n",
    "        file_path = Path(save_path) / file_name\n",
    "\n",
    "        try:\n",
    "            if file_path.exists():\n",
    "                print(f\"Already exists, skipping: {file_name}\")\n",
    "                id += 1\n",
    "                continue\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            extracted_sections = {\"url\": url}\n",
    "\n",
    "            main_title_tag = soup.find(\"h1\")\n",
    "            \n",
    "            if main_title_tag:\n",
    "                # Remove all <span> tags\n",
    "                for span in main_title_tag.find_all(\"span\"):\n",
    "                    span.decompose()\n",
    "                main_title = main_title_tag.get_text(strip=True)\n",
    "            else:\n",
    "                main_title = \"No Title\"\n",
    "            container = soup.find(\"div\", class_=\"field--name-field-component-sections\")\n",
    "\n",
    "            if not container:\n",
    "                print(f\"No component sections found in {url}\")\n",
    "                id += 1\n",
    "                continue\n",
    "\n",
    "            components = container.find_all(\"div\", class_=\"paragraph--type--component-section\")\n",
    "            for comp in components:\n",
    "                title_tag = comp.find(\"h2\", class_=\"component-section-section-title\")\n",
    "                content_tag = comp.find(\"div\", class_=\"field--name-field-component-section-content\")\n",
    "\n",
    "                if not content_tag:\n",
    "                    continue  \n",
    "\n",
    "                html_content = (\n",
    "                    content_tag.get_text(separator=\" \", strip=True)\n",
    "                    .replace(\"\\t\", \" \")\n",
    "                    .replace(\"\\xa0\", \" \")\n",
    "                    .replace(\" ,\", \",\")\n",
    "                    .replace(\"  \", \"\")\n",
    "                    .replace(\".,\", \",\")\n",
    "                    .strip()\n",
    "                )\n",
    "                if title_tag:\n",
    "                    title = title_tag.get_text(strip=True)\n",
    "                    extracted_sections[title] = html_content\n",
    "                else:\n",
    "                    extracted_sections[main_title] = html_content\n",
    "\n",
    "            # Save to JSON\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(extracted_sections, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Scraped and saved: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download or parse {url}: {e}\")\n",
    "        \n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19460f5-be25-4190-89fb-1d2f33b4046a",
   "metadata": {},
   "source": [
    "## 1. Asthma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e94e4d6-5c3e-4e80-9a38-c89315c0dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTHMA_RAW_DIR = Path(\"raw_files/asthma\")\n",
    "os.makedirs(ASTHMA_RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "72dd6d79-5d51-4468-a196-0ee9d643e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping: who_0.json\n",
      "Already exists, skipping: cdc_0.json\n",
      "Already exists, skipping: cdc_1.json\n",
      "Already exists, skipping: cdc_2.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_0.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_1.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_2.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_3.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_4.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_5.json\n",
      "Scraped and saved: raw_files\\asthma\\nih_6.json\n"
     ]
    }
   ],
   "source": [
    "WHO_scrapping(ASTHMA_RAW_DIR, \"asthma\", [\"overview\", \"impact\", \"symptoms\", \"causes\", \"treatment\", \"self-care\"])\n",
    "\n",
    "CDC_scrapping(ASTHMA_RAW_DIR, \"asthma/\", {\"about\": [\"symptoms\", \"diagnosis\", \"symptom management\"],\n",
    "                                          \"control\": [\"Common asthma triggers\"],\n",
    "                                          \"emergency\": [\"First steps\", \"Facing challenges\"]\n",
    "                                         })\n",
    "\n",
    "NIH_scrapping(ASTHMA_RAW_DIR, \"asthma/\", [\"\", \"symptoms\", \"attacks\", \"causes\", \"diagnosis\", \"treatment-action-plan\", \"living-with\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e83f2b-0929-4dba-8469-88dcc60ea7f9",
   "metadata": {},
   "source": [
    "## 2. COPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb86321a-1f3e-4583-a005-9ed16a0dca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "COPD_RAW_DIR = Path(\"raw_files/chronic-obstructive-pulmonary-disease-copd\")\n",
    "os.makedirs(COPD_RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d78fda7a-246e-4eaa-87ce-b9e85b57aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping: who_0.json\n",
      "Already exists, skipping: cdc_0.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_0.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_1.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_2.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_3.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_4.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_5.json\n",
      "Scraped and saved: raw_files\\chronic-obstructive-pulmonary-disease-copd\\nih_6.json\n"
     ]
    }
   ],
   "source": [
    "WHO_scrapping(COPD_RAW_DIR, \"chronic-obstructive-pulmonary-disease-(copd)\", \n",
    "              [\"overview\", \"symptoms\", \"causes\", \"treatment\", \"living with copd\"])\n",
    "\n",
    "CDC_scrapping(COPD_RAW_DIR, \"copd/\", {\"about\": [\"What it is\", \"symptoms\", \"Complications\", \"Causes and risk factors\", \"Reducing risk\", \n",
    "                                                \"Who is at risk\", \"Diagnosis\", \"Treatment and management\"]})\n",
    "\n",
    "NIH_scrapping(COPD_RAW_DIR, \"copd/\", [\"\", \"symptoms\", \"causes\", \"diagnosis\", \"prevention\", \"treatment\", \"living-with\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d919a7-ca6b-496b-a0ca-1b4f34ca314c",
   "metadata": {},
   "source": [
    "## 3. Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f9baada-5f2a-417b-8e54-3828e827a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNEUMONIA_RAW_DIR = Path(\"raw_files/pneumonia\")\n",
    "os.makedirs(PNEUMONIA_RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "983cf7f5-5e6c-437f-bdd8-805aff6136b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping: cdc_0.json\n",
      "Already exists, skipping: cdc_1.json\n",
      "Already exists, skipping: cdc_2.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_0.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_1.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_2.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_3.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_4.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_5.json\n",
      "Scraped and saved: raw_files\\pneumonia\\nih_6.json\n"
     ]
    }
   ],
   "source": [
    "# WHO_scrapping(COPD_RAW_DIR, \"chronic-obstructive-pulmonary-disease-(copd)\", \n",
    "#               [\"overview\", \"symptoms\", \"causes\", \"treatment\", \"living with copd\"])\n",
    "CDC_scrapping(PNEUMONIA_RAW_DIR, \"pneumonia/\", {\"about\": [\"Overview\", \"symptoms\", \"Types\", \"Who is at risk\", \"Causes\", \"Prevention\"],\n",
    "                                           \"risk-factors\": [\"People at increased risk\", \"Conditions that can increase risk\",\n",
    "                                                            \"Behaviors that can increase risk\"],\n",
    "                                           \"prevention\": [\"Prevention steps and strategies\"]\n",
    "                                          })\n",
    "\n",
    "NIH_scrapping(PNEUMONIA_RAW_DIR, \"pneumonia/\", [\"\", \"symptoms\", \"causes\", \"diagnosis\", \"prevention\", \"treatment\", \"recovery\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5cbaa2-1378-417a-92e5-eed65b987c71",
   "metadata": {},
   "source": [
    "## 4. Tuberculosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33707d9f-2f57-4943-ac61-1854548d302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUBERCULOSIS_RAW_DIR = Path(\"raw_files/tuberculosis\")\n",
    "os.makedirs(TUBERCULOSIS_RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c952249e-a4af-4f51-b203-de5f78d524ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping: who_0.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_0.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_1.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_2.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_3.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_4.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_5.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_6.json\n",
      "Scraped and saved: raw_files\\tuberculosis\\cdc_7.json\n"
     ]
    }
   ],
   "source": [
    "WHO_scrapping(TUBERCULOSIS_RAW_DIR, \"tuberculosis\", \n",
    "              [\"overview\", \"symptoms\", \"treatment\", \"prevention\", \"diagnosis\", \"impact\"])\n",
    "\n",
    "CDC_scrapping(TUBERCULOSIS_RAW_DIR, \"tb/\", {\"about\": [\"Overview\", \"Signs and symptoms\", \"Types\", \"Risk factors\", \"How it spreads\", \n",
    "                                                      \"Prevention\", \"Testing\", \"Treatment\", \"Vaccines\"],\n",
    "                                           \"signs-symptoms\": [\"Signs and symptoms\"],\n",
    "                                           \"causes\": [\"Causes\", \"How it spreads\"],\n",
    "                                           \"vaccines\": [\"Overview\"],\n",
    "                                           \"testing\": [\"Types of tests\", \"Why get tested\", \"Who should be tested\", \"What to do if you've tested positive\"],\n",
    "                                           \"exposure\": [\"Contact your health care provider if you have been exposed to TB\",\n",
    "                                                        \"Only persons with active TB disease can spread TB to others\",\n",
    "                                                        \"Contact investigations can help limit the spread of TB\"],\n",
    "                                           \"risk-factors\": [\"Places with increased risk\", \"Conditions that can increase risk\"],\n",
    "                                           \"prevention\": [\"Prevention steps and strategies\"]\n",
    "                                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff4f01-728b-4e90-8259-51685c90b9cb",
   "metadata": {},
   "source": [
    "## 5. Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "194ba0d3-0f8f-4e3d-abc1-2573929dc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_RAW_DIR = Path(\"raw_files/coronavirus-disease-(covid-19)\")\n",
    "os.makedirs(COVID_RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "de3303cb-5c8b-47f2-94d8-00ce122aef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping: who_0.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_0.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_1.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_2.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_3.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_4.json\n",
      "Scraped and saved: raw_files\\coronavirus-disease-(covid-19)\\cdc_5.json\n"
     ]
    }
   ],
   "source": [
    "WHO_scrapping(COVID_RAW_DIR, \"coronavirus-disease-(covid-19)\", \n",
    "              [\"overview\", \"symptoms\", \"treatment\", \"prevention\"])\n",
    "\n",
    "CDC_scrapping(COVID_RAW_DIR, \"covid/\", {\"about\": [\"Learn about COVID-19 and how it spreads\"],\n",
    "                                        \"signs-symptoms\": [\"Signs and symptoms\", \"When to seek emergency help\", \"Difference between flu and COVID-19\"],\n",
    "                                        \"risk-factors\": [\"Overview\", \"Conditions that can increase risk\"],\n",
    "                                        \"testing\": [\"Types of tests\", \"Choosing a COVID-19 test\", \"Interpreting your results\"],\n",
    "                                        \"treatment\": [\"COVID-19 Treatment Options\", \"Preventing COVID-19\"],\n",
    "                                        \"prevention\": [\"Core Prevention Strategies\", \"What to watch out for\"]\n",
    "                                        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG)",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
